k_e_2 <- S4[6,2]*1000
k_edis_2 <- S4[7,2]*1000
# D. magna parameters (#3)
k_u1_3 <- S4[1,3]/1000
k_u2_3 <- S4[2,3]/1000/d
k_u3_3 <- S4[3,3]/1000
k_d_3 <- S4[4,3]*1000
a_enm_3 <- S4[5,3]
k_e_3 <- S4[6,3]*1000
k_edis_3 <- S4[7,3]*1000
# H. azteca parameters (#4)
k_u1_4 <- S4[1,4]/1000
k_u2_4 <- S4[2,4]/1000/d
k_u3_4 <- S4[3,4]/1000
k_d_4 <- S4[4,4]*1000
a_enm_4 <- S4[5,4]
k_e_4 <- S4[6,4]*1000
k_edis_4 <- S4[7,4]*1000
# V. constricta parameters (#5)
k_u1_5 <- S4[1,5]/1000
k_u2_5 <- S4[2,5]/1000/d
k_u3_5 <- S4[3,5]/1000
k_d_5 <- S4[4,5]*1000
a_enm_5 <- S4[5,5]
k_e_5 <- S4[6,5]*1000
k_edis_5 <- S4[7,5]*1000
# P. promelas parameters (#6)
k_u1_6 <- S4[1,6]/1000
k_u2_6 <- S4[2,6]/1000/d
k_u3_6 <- S4[3,6]/1000
k_d_6 <- S4[4,6]*1000
a_enm_6 <- S4[5,6]
k_e_6 <- S4[6,6]*1000
k_edis_6 <- S4[7,6]*1000
# O. mykiss parameters (#7)
k_u1_7 <- S4[1,7]/1000
k_u2_7 <- S4[2,7]/1000/d
k_u3_7 <- S4[3,7]/1000
k_d_7 <- S4[4,7]*1000
a_enm_7 <- S4[5,7]
k_e_7 <- S4[6,7]*1000
k_edis_7 <- S4[7,7]*1000
return(list("Vw" = Vw, "Vss" = Vss, "Vsed" = Vsed, "Vsedw" = Vsedw,
"k_dis" = kdis, "a_bio" = a_bio,
"M_1" = M_1, "L_1" = L_1, "B_1" = B_1, "pH_1" = pH_1, "a_1" = a_1,
"M_2" = M_2, "L_2" = L_2, "B_2" = B_2, "pH_2" = pH_2, "a_2" = a_2,
"M_3" = M_3, "L_3" = L_3, "B_3" = B_3, "pH_3" = pH_3, "a_3" = a_3,
"M_4" = M_4, "L_4" = L_4, "B_4" = B_4, "pH_4" = pH_4, "a_4" = a_4,
"M_5" = M_5, "L_5" = L_5, "B_5" = B_5, "pH_5" = pH_5, "a_5" = a_5,
"M_6" = M_6, "L_6" = L_6, "B_6" = B_6, "pH_6" = pH_6, "a_6" = a_6,
"M_7" = M_7, "L_7" = L_7, "B_7" = B_7, "pH_7" = pH_7, "a_7" = a_7,
"k_u1_1"=k_u1_1, "k_u2_1"=k_u2_1, "k_u3_1"=k_u3_1, "k_d_1"=k_d_1, "a_enm_1"=a_enm_1, "k_e_1"=k_e_1, "k_edis_1"=k_edis_1,
"k_u1_2"=k_u1_2, "k_u2_2"=k_u2_2, "k_u3_2"=k_u3_2, "k_d_2"=k_d_2, "a_enm_2"=a_enm_2, "k_e_2"=k_e_2, "k_edis_2"=k_edis_2,
"k_u1_3"=k_u1_3, "k_u2_3"=k_u2_3, "k_u3_3"=k_u3_3, "k_d_3"=k_d_3, "a_enm_3"=a_enm_3, "k_e_3"=k_e_3, "k_edis_3"=k_edis_3,
"k_u1_4"=k_u1_4, "k_u2_4"=k_u2_4, "k_u3_4"=k_u3_4, "k_d_4"=k_d_4, "a_enm_4"=a_enm_4, "k_e_4"=k_e_4, "k_edis_4"=k_edis_4,
"k_u1_5"=k_u1_5, "k_u2_5"=k_u2_5, "k_u3_5"=k_u3_5, "k_d_5"=k_d_5, "a_enm_5"=a_enm_5, "k_e_5"=k_e_5, "k_edis_5"=k_edis_5,
"k_u1_6"=k_u1_6, "k_u2_6"=k_u2_6, "k_u3_6"=k_u3_6, "k_d_6"=k_d_6, "a_enm_6"=a_enm_6, "k_e_6"=k_e_6, "k_edis_6"=k_edis_6,
"k_u1_7"=k_u1_7, "k_u2_7"=k_u2_7, "k_u3_7"=k_u3_7, "k_d_7"=k_d_7, "a_enm_7"=a_enm_7, "k_e_7"=k_e_7, "k_edis_7"=k_edis_7,
"C_water"=C_water, "C_water_dis"=C_water_dis, "C_sed"=C_sed, "C_ss"=C_ss, "times"=times))
}
params <- create.params(material)
### Function to create initial values for ODEs ###
create.inits <- function(parameters){
with( as.list(parameters),{
Cb_1<-0; Cdis_1<-0; Cb_2<-0; Cdis_2<-0; Cb_3<-0; Cdis_3<-0; Cb_4<-0; Cdis_4<-0;
Cb_5<-0; Cdis_5<-0; Cb_6<-0; Cdis_6<-0; Cb_7<-0; Cdis_7<-0;
Cw<-0.0001; Cw_dis<-0;Csed<-0; Css<-0;
return(c("Cb_1"=Cb_1, "Cdis_1"=Cdis_1, "Cb_2"=Cb_2, "Cdis_2"=Cdis_2,
"Cb_3"=Cb_3, "Cdis_3"=Cdis_3, "Cb_4"=Cb_4, "Cdis_4"=Cdis_4,
"Cb_5"=Cb_5, "Cdis_5"=Cdis_5, "Cb_6"=Cb_6, "Cdis_6"=Cdis_6,
"Cb_7"=Cb_7, "Cdis_7"=Cdis_7, "Cw" = Cw, "Cw_dis"=Cw_dis,
"Csed"=Csed, "Css"=Css))
})
}
inits <- create.inits(params)
### Function to create events ###
create.events <- function(parameters){
with( as.list(parameters), {
lCw <- length(C_water)
lCw_dis <- length(C_water_dis)
lCsed <- length(C_sed)
lCss <- length(C_ss)
ltimes <- length(times)
if (lCw != lCw_dis | lCw != lCsed | lCw != lCss | lCw != ltimes){
stop("The length of C_water, C_water_dis, C_sed, C_ss and times must be equal.")
}else{
events <- list(data = data.frame(var = c(rep("Cw", lCw), rep("Cw_dis", lCw_dis), rep("Csed", lCsed), rep("Css", lCss)),
time = c(rep(times,4)),
value = c(C_water, C_water_dis, C_sed, C_ss),
method = c(rep("rep",4))))
}
return(events)
})
}
events <- create.events(params)
### ODEs system ###
ode.func <- function(time, inits, params){
with(as.list(c(inits, params)),{
# S. capricornutum #1
D_1 <- 1/L_1
dCb_1 <- k_u1_1*Cw - k_e_1*Cb_1 - k_dis*Cb_1 #- D_1*Cb_1
dCdis_1 <- k_u3_1*a_bio*Cw_dis + k_dis*Cb_1 - k_edis_1*Cdis_1 #- D_1*Cdis_1
# F. crotonensis #2
D_2 <- 1/L_2
dCb_2 <- k_u1_2*Cw - k_e_2*Cb_2 - k_dis*Cb_2 #- D_2*Cb_2
dCdis_2 <- k_u3_2*a_bio*Cw_dis +k_dis*Cb_2 - k_edis_2*Cdis_2# - D_2*Cdis_2
# D. magna #3
D_3 <- 1/L_3
dCb_3 <- k_u1_3*Cw + k_u2_3*Css + a_3*a_enm_3*k_d_3*Cb_1 - k_e_3*Cb_3 - k_dis*Cb_3 #- D_3*Cb_3
dCdis_3 <- k_u3_3*a_bio*Cw_dis + k_dis*Cb_3 - k_edis_3*Cdis_3 #- D_3*Cdis_3
# H. azteca #4
D_4 <- 1/L_4
dCb_4 <- k_u1_4*Cw + k_u2_4*Csed + a_4*a_enm_4*k_d_4*Cb_1 - k_e_4*Cb_4 - k_dis*Cb_4 #- D_4*Cb_4
dCdis_4 <- k_u3_4*a_bio*Cw_dis + k_dis*Cb_4 - k_edis_4*Cdis_4 #- D_4*Cdis_4
# V. constricta #5
D_5 <- 1/L_5
dCb_5 <- k_u1_5*Cw + k_u2_5*Csed +a_5*a_enm_5*k_d_5*Cb_1 - k_e_5*Cb_5 - k_dis*Cb_5 #- D_5*Cb_5
dCdis_5 <- k_u3_5*a_bio*Cw_dis + k_dis*Cb_5 - k_edis_5*Cdis_5 #- D_5*Cdis_5
# P. promelas #6
D_6 <- 1/L_6
dCb_6 <- k_u1_6*Cw + a_6*a_enm_6*k_d_6*Cb_3 - k_e_6*Cb_6 - k_dis*Cb_6# - D_6*Cb_6
dCdis_6 <- k_u3_6*a_bio*Cw_dis + k_dis*Cb_6 - k_edis_6*Cdis_6 #- D_6*Cdis_6
# O. mykiss #7
D_7  <- 1/L_7
dCb_7 <- k_u1_7*Cw + a_7*a_enm_7*k_d_7*Cb_6 - k_e_7*Cb_7 -k_dis*Cb_7# - D_7*Cb_7
dCdis_7 <- k_u3_7*a_bio*Cw_dis + k_dis*Cb_7 - k_edis_7*Cdis_7 #- D_7*Cdis_7
#Freshwater
# ENM
dCw <- 0
# Dissolved
dCw_dis <- 0
# Sediment
dCsed <- 0
# Suspended Sediment
dCss <- 0
list(c(dCb_1=dCb_1, dCdis_1=dCdis_1, dCb_2=dCb_2, dCdis_2=dCdis_2,
dCb_3=dCb_3, dCdis_3=dCdis_3, dCb_4=dCb_4, dCdis_4=dCdis_4,
dCb_5=dCb_5, dCdis_5=dCdis_5, dCb_6=dCb_6, dCdis_6=dCdis_6,
dCb_7=dCb_7, dCdis_7=dCdis_7, dCw = dCw, dCw_dis=dCw_dis,
dCsed=dCsed, dCss=dCss))
})
}
sample_time <- seq(0,1,0.001) # in days
end_time <- 14
keep_solution <- matrix(rep(NA,14*14), ncol = 14)
L <- c(2,2,3,3,60,60,365,365,3650,3650,912.5, 912.5, 2920, 2920)
for (i in 1:end_time){
if (i == 1){
solution <- ode(times = sample_time, func = ode.func, y = inits, parms = params, method = "lsodar", rtol =1e-09, atol = 1e-9)#, events = events)
}else{
solution <- ode(times = sample_time, func = ode.func, y = keep_solution[i-1,], parms = params, method = "lsodar", rtol =1e-09, atol = 1e-9)#, events = events)
}
keep_solution[i,] <-  solution[dim(solution)[1],]*L
}
L <- c(2,2,3,3,60,60,365,365,3650,3650,912.5, 912.5, 2920, 2920)
(1-(1/L))
c((1-(1/L)),1,1,1,1)
sample_time <- seq(0,1,0.001) # in days
end_time <- 14
keep_solution <- matrix(rep(NA,end_time*18), ncol = 18)
L <- c(2,2,3,3,60,60,365,365,3650,3650,912.5, 912.5, 2920, 2920)
for (i in 1:end_time){
if (i == 1){
solution <- ode(times = sample_time, func = ode.func, y = inits, parms = params, method = "lsodar", rtol =1e-09, atol = 1e-9)#, events = events)
}else{
solution <- ode(times = sample_time, func = ode.func, y = keep_solution[i-1,], parms = params, method = "lsodar", rtol =1e-09, atol = 1e-9)#, events = events)
}
keep_solution[i,] <-  solution[dim(solution)[1],1:14]*c((1-(1/L)),1,1,1,1)
}
sample_time <- seq(0,1,0.001) # in days
end_time <- 14
keep_solution <- matrix(rep(NA,end_time*18), ncol = 18)
colnames(keep_solution) <- c("Cb_1", "Cdis_1", "Cb_2", "Cdis_2",
"Cb_3", "Cdis_3", "Cb_4", "Cdis_4",
"Cb_5", "Cdis_5","Cb_6", "Cdis_6",
"Cb_7", "Cdis_7", "Cw", "Cw_dis",
"Csed", "Css")
L <- c(2,2,3,3,60,60,365,365,3650,3650,912.5, 912.5, 2920, 2920)
for (i in 1:end_time){
if (i == 1){
solution <- ode(times = sample_time, func = ode.func, y = inits, parms = params, method = "lsodar", rtol =1e-09, atol = 1e-9)#, events = events)
}else{
solution <- ode(times = sample_time, func = ode.func, y = keep_solution[i-1,], parms = params, method = "lsodar", rtol =1e-09, atol = 1e-9)#, events = events)
}
keep_solution[i,] <-  solution[dim(solution)[1],1:14]*c((1-(1/L)),1,1,1,1)
}
warnings()
sample_time <- seq(0,1,0.001) # in days
end_time <- 14
keep_solution <- matrix(rep(NA,end_time*18), ncol = 18)
colnames(keep_solution) <- c("Cb_1", "Cdis_1", "Cb_2", "Cdis_2",
"Cb_3", "Cdis_3", "Cb_4", "Cdis_4",
"Cb_5", "Cdis_5","Cb_6", "Cdis_6",
"Cb_7", "Cdis_7", "Cw", "Cw_dis",
"Csed", "Css")
L <- c(2,2,3,3,60,60,365,365,3650,3650,912.5, 912.5, 2920, 2920)
for (i in 1:end_time){
if (i == 1){
solution <- ode(times = sample_time, func = ode.func, y = inits, parms = params, method = "lsodar", rtol =1e-09, atol = 1e-9)#, events = events)
}else{
solution <- ode(times = sample_time, func = ode.func, y = keep_solution[i-1,], parms = params, method = "lsodar", rtol =1e-09, atol = 1e-9)#, events = events)
}
keep_solution[i,] <-  solution[dim(solution)[1],]*c((1-(1/L)),1,1,1,1)
}
dim(solution)
sample_time <- seq(0,1,0.001) # in days
end_time <- 14
keep_solution <- matrix(rep(NA,end_time*18), ncol = 18)
colnames(keep_solution) <- c("Cb_1", "Cdis_1", "Cb_2", "Cdis_2",
"Cb_3", "Cdis_3", "Cb_4", "Cdis_4",
"Cb_5", "Cdis_5","Cb_6", "Cdis_6",
"Cb_7", "Cdis_7", "Cw", "Cw_dis",
"Csed", "Css")
L <- c(2,2,3,3,60,60,365,365,3650,3650,912.5, 912.5, 2920, 2920)
for (i in 1:end_time){
if (i == 1){
solution <- ode(times = sample_time, func = ode.func, y = inits, parms = params, method = "lsodar", rtol =1e-09, atol = 1e-9)#, events = events)
}else{
solution <- ode(times = sample_time, func = ode.func, y = keep_solution[i-1,], parms = params, method = "lsodar", rtol =1e-09, atol = 1e-9)#, events = events)
}
keep_solution[i,] <-  solution[dim(solution)[1],2:19]*c((1-(1/L)),1,1,1,1)
}
transformed_solution <-cbind(keep_solution*10^9)  #  transform kg ENM/mg body to mg ENM/g body
print(transformed_solution)
library(bnlearn)
library(yardstick)
library(tidyverse)
# Here we use Mathews Correlation Coefficient to compare the different methodologies.
# It is regarded as a balanced measure which can be used even if the classes are of
# very different sizes.  A coefficient of +1 represents a perfect prediction, 0 no better
# than random prediction and -1 indicates total disagreement between prediction and observation.
# When there are more than two labels the MCC will no longer range between -1 and +1. Instead the
# minimum  value will be between -1 and 0 depending on the true distribution.
# The maximum value is always +1. Another possible comparison metric is the Cohen's Kappa.
data <- openxlsx::read.xlsx("C:/Users/user/Desktop/Jaqpot/R/bnlearn/NM_data.xlsx",sheet=6, colNames=T)
#convert all columns to factors
data[] <- lapply(data, as.factor) #lapply returns list so [] keeps the initial data format (data.frame)
# Remove variables with only one category
drops <- c("Dissolution","Immunological.effects")
data <- data[ , !(names(data) %in% drops)]
#Check the structure of our dataset
str(data)
df_levels <- list()
for (j in 1:dim(data)[2]){
df_levels[[j]] <- levels(data[,j])
}
# Shuffle data
set.seed(1298)
data <- data[sample(nrow(data)),]
N_folds <- 10
N_data  <- dim(data)[1]
step  <- round(N_data/N_folds)
#create an empty network
ug <- bnlearn::empty.graph(names(data))
#add the arcs by assigning a two-column matrix containing the labels of their end-nodes.
#Undirected arcs are represented as their two possible orientations
bnlearn::arcs(ug, check.cycles = TRUE) = matrix(c( "NM.Hazard","Shape", "NM.Hazard","Nanoparticle",
"NM.Hazard", "Surface.area",  "NM.Hazard",
"Surface.charge",  "NM.Hazard","Surface.coatings",  "NM.Hazard",
"Surface.reactivity",  "NM.Hazard", "Aggregation",  "NM.Hazard",
"Particle.size",  "NM.Hazard","Administration.route",  "NM.Hazard",
"Study.type",  "NM.Hazard","Cytotoxicity",  "NM.Hazard",
"Neurological.effects",  "NM.Hazard", "Pulmonary.effects",  "NM.Hazard",
"Fibrosis",  "NM.Hazard", "RCNS.effects",   "NM.Hazard", "Genotoxicity",  "NM.Hazard",
"Inflammation", "Nanoparticle","Shape",
"Nanoparticle",  "Surface.reactivity", "Nanoparticle", "Neurological.effects",
"Nanoparticle", "Surface.coatings","Nanoparticle", "Surface.charge",
"Nanoparticle", "Administration.route", "Nanoparticle",  "Fibrosis",
"Shape","Genotoxicity","Surface.area", "Neurological.effects",
"Surface.coatings", "Surface.area",  "Surface.coatings",  "Particle.size",
"Surface.coatings", "Cytotoxicity", "Surface.coatings", "Pulmonary.effects",
"Surface.coatings", "Aggregation",   "Surface.coatings",  "Study.type",
"Pulmonary.effects", "Inflammation","Inflammation","RCNS.effects"),
ncol = 2, byrow = TRUE, dimnames = list(c(), c("from", "to")))
start <- 1
# Test cross-val accuracy of best model
for(i in 1: N_folds){
select <- start:(start+step-1)
if(i ==10){
select <- start:559
}
test <- data[select,]
#Extract observations
observed <- test$NM.Hazard
train <- data[-select,]
# fit the model
training.fit  <- bnlearn::bn.fit(ug, train, method="bayes") #method = "bayes"/"mle"
# Delete all observed nodes
for (k in 1:dim(test)[1]){
for (j in 11:18){
test[k,j] <- NA
}
}
N_test <- dim(test)[1]
pred <- rep(0, N_test)
# Convert all results to factors in order to calculate the confusion matrix
pred <- as.factor(pred)
observed <- as.factor(observed)
levels(pred) <- levels(observed)
for (j in 1:N_test){
# n : number of samples used to average
pred[j] <-  as.character(predict(training.fit, data=test[j,!is.na(test[j,])],
node= "NM.Hazard",prob = T, method = "bayes-lw", n=10000))
#method = "bayes-lw" / "parents"
}
mcc[[i]]  <- yardstick::mcc_vec(observed, pred)
conf[[i]]  <- tibble::tibble(truth = observed,
estimate = pred) %>%
conf_mat(truth, estimate)
acc[[i]]  <-  yardstick::accuracy_vec(observed, pred)#  # Update starting position of test
start <- start+step
}
jaqpotr
deploy.bn(trained.model = training.fit)
jaqpotr::deploy.bn(trained.model = training.fit)
devtools::install_github("euclia/jaqpotr")
jaqpotr::deploy.bn(trained.model = training.fit)
library(bnlearn)
library(yardstick)
library(tidyverse)
# Here we use Mathews Correlation Coefficient to compare the different methodologies.
# It is regarded as a balanced measure which can be used even if the classes are of
# very different sizes.  A coefficient of +1 represents a perfect prediction, 0 no better
# than random prediction and -1 indicates total disagreement between prediction and observation.
# When there are more than two labels the MCC will no longer range between -1 and +1. Instead the
# minimum  value will be between -1 and 0 depending on the true distribution.
# The maximum value is always +1. Another possible comparison metric is the Cohen's Kappa.
data <- openxlsx::read.xlsx("C:/Users/user/Desktop/Jaqpot/R/bnlearn/NM_data.xlsx",sheet=6, colNames=T)
#convert all columns to factors
data[] <- lapply(data, as.factor) #lapply returns list so [] keeps the initial data format (data.frame)
# Remove variables with only one category
drops <- c("Dissolution","Immunological.effects")
data <- data[ , !(names(data) %in% drops)]
#Check the structure of our dataset
str(data)
df_levels <- list()
for (j in 1:dim(data)[2]){
df_levels[[j]] <- levels(data[,j])
}
# Shuffle data
set.seed(1298)
data <- data[sample(nrow(data)),]
N_folds <- 10
N_data  <- dim(data)[1]
step  <- round(N_data/N_folds)
#create an empty network
ug <- bnlearn::empty.graph(names(data))
#add the arcs by assigning a two-column matrix containing the labels of their end-nodes.
#Undirected arcs are represented as their two possible orientations
bnlearn::arcs(ug, check.cycles = TRUE) = matrix(c( "NM.Hazard","Shape", "NM.Hazard","Nanoparticle",
"NM.Hazard", "Surface.area",  "NM.Hazard",
"Surface.charge",  "NM.Hazard","Surface.coatings",  "NM.Hazard",
"Surface.reactivity",  "NM.Hazard", "Aggregation",  "NM.Hazard",
"Particle.size",  "NM.Hazard","Administration.route",  "NM.Hazard",
"Study.type",  "NM.Hazard","Cytotoxicity",  "NM.Hazard",
"Neurological.effects",  "NM.Hazard", "Pulmonary.effects",  "NM.Hazard",
"Fibrosis",  "NM.Hazard", "RCNS.effects",   "NM.Hazard", "Genotoxicity",  "NM.Hazard",
"Inflammation", "Nanoparticle","Shape",
"Nanoparticle",  "Surface.reactivity", "Nanoparticle", "Neurological.effects",
"Nanoparticle", "Surface.coatings","Nanoparticle", "Surface.charge",
"Nanoparticle", "Administration.route", "Nanoparticle",  "Fibrosis",
"Shape","Genotoxicity","Surface.area", "Neurological.effects",
"Surface.coatings", "Surface.area",  "Surface.coatings",  "Particle.size",
"Surface.coatings", "Cytotoxicity", "Surface.coatings", "Pulmonary.effects",
"Surface.coatings", "Aggregation",   "Surface.coatings",  "Study.type",
"Pulmonary.effects", "Inflammation","Inflammation","RCNS.effects"),
ncol = 2, byrow = TRUE, dimnames = list(c(), c("from", "to")))
start <- 1
# Test cross-val accuracy of best model
for(i in 1: N_folds){
select <- start:(start+step-1)
if(i ==10){
select <- start:559
}
test <- data[select,]
#Extract observations
observed <- test$NM.Hazard
train <- data[-select,]
# fit the model
training.fit  <- bnlearn::bn.fit(ug, train, method="bayes") #method = "bayes"/"mle"
# Delete all observed nodes
for (k in 1:dim(test)[1]){
for (j in 11:18){
test[k,j] <- NA
}
}
N_test <- dim(test)[1]
pred <- rep(0, N_test)
# Convert all results to factors in order to calculate the confusion matrix
pred <- as.factor(pred)
observed <- as.factor(observed)
levels(pred) <- levels(observed)
for (j in 1:N_test){
# n : number of samples used to average
pred[j] <-  as.character(predict(training.fit, data=test[j,!is.na(test[j,])],
node= "NM.Hazard",prob = T, method = "bayes-lw", n=10000))
#method = "bayes-lw" / "parents"
}
mcc[[i]]  <- yardstick::mcc_vec(observed, pred)
conf[[i]]  <- tibble::tibble(truth = observed,
estimate = pred) %>%
conf_mat(truth, estimate)
acc[[i]]  <-  yardstick::accuracy_vec(observed, pred)#  # Update starting position of test
start <- start+step
}
jaqpotr::deploy.bn(trained.model = training.fit)
object <- jsonlite::fromJSON("C:/Users/user/Desktop/Jaqpot/R/json/bnlearn.json")
dataset <- object$dataset
rawModel <- object$rawModel
additionalInfo <- object$additionalInfo
#################################
## Input retrieval from Jaqpot ##
#################################
# Get feature keys (a key number that points to the url)
feat.keys <-  dataset$features$key
# Get feature names (actual name)
feat.names <- dataset$features$name
# Create a dataframe that includes the feature key and the corresponding name
key.match <- data.frame(cbind(feat.keys, feat.names))
# Convert factor to string (feat.names is converted factor by data.frame())
key.match[] <- lapply(key.match, as.character)
# Initialize a dataframe with as many rows as the number of values per feature
rows_data <- length(dataset$dataEntry$values[,2])
df <- data.frame(matrix(0, ncol = 0, nrow = rows_data))
for(key in feat.keys){
# For each key (feature) get the vector of values (of length 'row_data')
feval <- dataset$dataEntry$values[key][,1]
# Name the column with the corresponding name that is connected with the key
df[key.match[key.match$feat.keys == key, 2]] <- feval
}
# Convert "NA" to NA
for (i in 1:dim(df)[1]){
for (j in 1:dim(df)[2]){
if(!is.na(df[i,j])){
if(df[i,j] == "NA"){
df[i,j] <- NA
}
}
}
}
# Extract the predicted value names
predicted.feats <- rep(0,  length(additionalInfo$predictedFeatures))
for (i in 1:length(predicted.feats)){
predicted.feats[i] <- additionalInfo$predictedFeatures[[i]]
}
###########################
## Model unserialization ##
###########################
mod <- unserialize(jsonlite::base64_dec(rawModel))
model <- mod$MODEL
method <- additionalInfo$fromUser$predict.args$method
n <- additionalInfo$fromUser$predict.args$n
# Convert categorical variables to factors
for (i in 1:dim(df)[2]){
varname <- names(df)[i]
if(varname !=  "query node"){
# Retrieve the levels from the model and convert to factor
df[,i] <- factor(df[,i], levels = attributes(model[[eval(expr(varname))]]$prob)$dimnames[[1]])
}
}
################
## Prediction ##
################
#Define a parameter to store thenumber of levels of each query nodes
level.count <- rep(NA,dim(df)[1] )
# Get output dimensions depending on the number of levels of each query node
for (instance in 1:dim(df)[1]){
pred.node <- as.character(df$`query node`[instance])
level.count[instance] <- length(levels(df[[eval(expr(pred.node))]]))
}
# Create a matrix to store the solution
prediction <- matrix(rep(NA,3*sum(level.count)), ncol = 3)
# Create a variable to store the last row filled in the prediction matrix
where <- 1
for (instance in 1:dim(df)[1]){
pred.node <- as.character(df$`query node`[instance])
#Provide only non NA values
newdata <-  df[instance,!is.na(df[instance,])]
# Remove the query node
newdata <-  newdata[ ,-which(names(newdata) == "query node")]
# Store in the first column the query node
prediction[where:(where+level.count[instance]-1),1] <- pred.node
# Make the prediction
result = attributes(predict(model, data = newdata, node= pred.node, prob = T, method = method, n = n))$prob[,1]
# Store in the seconde column the levels
prediction[where:(where+level.count[instance]-1),2] <- names(result)
# Store the probabilities returned by the model in the third column
prediction[where:(where+level.count[instance]-1),3] <- round(result,3)
#Update the row count
where <- where+level.count[instance]
}
colnames(prediction) <- c("query node", "prediction class", "probability")
##################################
## Name and return predictions  ##
##################################
for(i in 1:dim(prediction)[1]){
pred<- data.frame((t(prediction[i,])))
# Bring everything into a format that cooperates with Jaqpot
if(i==1){lh_preds<- list(jsonlite::unbox(pred))
}else{
lh_preds[[i]]<- jsonlite::unbox(pred)
}
}
datpred <-list(predictions=lh_preds)
return(datpred)
datpred
predicted.feats
c("query node", "prediction class", "probability") == c("query node", "prediction class", "probability")
getwd()
setwd("C:/Users/user/Documents/GitHub/GenericR")
devtools::build()
opencpu::ocpu_start_app("euclia/GenericR")
