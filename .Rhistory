if(!is.null(replace)){
#Convert character to numeric
if(!is.na(as.numeric(replace[2]))){
replace.value <- as.numeric(replace[2])
}else{
replace.value <- replace[2]
}
}
####################
## Preprocessing ##
####################
# Do the NA substitution before preprocessing, if "before" is provided by the user
if(!is.null(replace)){
if(replace[1] == "before"){
for (i in 1:dim(df)[1]){
for (j in 1:dim(df)[2]){
if(is.na(df[i,j])){
df[i,j] <- replace.value
}
}
}
}
}
# If there is a preprocess stage apply it, else just predict
if(!is.null(preprocess)){
#if there is a preprocess model that is dummy vars, then retrieve factors
ModelForNames = NULL
for(i in 1:length(preprocess)){
if(attributes((preprocess[[i]]))$class == "dummyVars"){
ModelForNames <- preprocess[[i]]
# Retrive the original classes of the dataset's categorical vars
for (i in 1:dim(df)[2]){
#Retrieve levels of factor
if( attr(ModelForNames$terms, "dataClasses")[colnames(df)[i]] == "factor"){
df[,i] <- factor(df[,i], levels = ModelForNames$lvls[colnames(df)[i]][[1]])
}
}
}
}
# If there is no dummy vars, use the first model to retrieve factors
if(is.null(ModelForNames)){
# If it's ensemble, there are multiple models, so use one for getting the categorical variables
if(!is.null(ensemble)){
ModelForNames <- model[[1]]
}else{
ModelForNames <- model
}
# Retrive the original classes of the dataset's categorical vars
for (i in 1:dim(df)[2]){
#Retrieve levels of factor
if( attr(ModelForNames$terms, "dataClasses")[colnames(df)[i]] == "factor"){
df[,i] <- factor(df[,i], levels = ModelForNames$xlevels[colnames(df)[i]][[1]])
}
}
}
#Data preprocessing
for (i in 1:length(preprocess)){
preprocess.method <- preprocess[[i]]
preprocessData <- predict(preprocess.method, df)
df <- preprocessData
}
}
# Do the NA substitution after preprocessing, if "after" is provided by the user
if(length(replace) == 2){
if(replace[1] == "after"){
for (i in 1:dim(df)[1]){
for (j in 1:dim(df)[2]){
if(is.na(df[i,j])){
df[i,j] <- replace.value
}
}
}
}
}
################
## Prediction ##
################
# Retrieve ensemble column names
ensemble.vars <- additionalInfo$fromUser$ensemble.vars
if(!is.null(ensemble)){
stacking <-  matrix(rep(NA, length(model)* dim(df)[1]), ncol = length(model))
for (i in 1:length(model)){
# Select the model to use
UseModel <- model[[i]]
stacking[,i] <- predict(UseModel, df)
}
# Convert the stacked predictions matrix to dataframe and name the columns of the dataset appropriately
stacking <- as.data.frame(stacking)
colnames(stacking) <- ensemble.vars
# Predict using the ensemble model
predictions <- predict(ensemble, stacking)
}else{
predictions <- predict(model, df)
}
######################
## Detransformation ##
######################
#Apply detransformation
ymin <- additionalInfo$fromUser$ymin
ymax <- additionalInfo$fromUser$ymax
if(length(ymin) == 1 && length(ymax) ==1){
for (i in 1:length(predictions)){
predictions[i] <- predictions[i]*(ymax-ymin)+ ymin
}
}
# Not offered to users (custom detransformation)
if (!is.null(extra.args)){
predictions <- extra.args(predictions)
}
##################################
## Name and return predictions  ##
##################################
for(i in 1:length(predictions)){
prediction<- data.frame(predictions[i])
colnames(prediction)<- predFeat
if(i==1){lh_preds<- list(jsonlite::unbox(prediction))
}else{
lh_preds[[i]]<- jsonlite::unbox(prediction)
}
}
datpred <-list(predictions=lh_preds)
datpred
df
str(df)
View(df)
datpred
setwd("C:/Users/user/Downloads/code_DOX_1a/code_DOX_1a")
load("C:/Users/user/Downloads/code_DOX_1a/code_DOX_1a/code_DOX_1a.RData")
load("C:/Users/user/Downloads/code_DOX_1a/code_DOX_1a/code_DOX_1a.RData")
View(DATA1Train)
## R version 4.1.2.(previously done with 3.5.1, same results)
#INSTALL PACKAGES
#if (!requireNamespace("BiocManager", quietly = TRUE))
# install.packages("BiocManager")
#BiocManager::install("Biobase", version = "3.8")
#install.packages("caret")
#install.packages("caretEnsemble")
#install.packages("randomForest")
#install.packages("tidyverse")
#install.packages("mlbench")
#install.packages("corrplot")
#install.packages("xgboost")
#install.packages("rpart")
#install.packages ("rpart.plot")
#install.packages ("dplyr")
#install.packages ("magrittr")
#install.packages("WVPlots")
#install.packages('lime')
#install.packages('MASS')
#install.packages("neuralnet")
#install.packages("PerformanceAnalytics")
#LOAD PACKAGES
library(tidyverse)
library(dplyr)
library(magrittr)
library(mlbench)
library(caret)
library(caretEnsemble)
library(randomForest)
library(corrplot)
library(xgboost)
library(rpart)
library (rpart.plot)
library(WVPlots)
library(MASS)
library(neuralnet)
library(PerformanceAnalytics)
## R version 3.5.1
## SET WORKING DIRECTORY
#setwd("~/../Dropbox/DOXORUBICIN/NEW PROJECT DOX/DRUG RELEASE DATA/CODE_DOX/code_DOX_1a ") ### you can set your own working directory
#### FUNCTIONS FOR MODEL STATISTICS
rmse = function(m, o){
sqrt(mean((m - o)^2))
}
## Rsquared FUNCTION (2 DIFFERENT WAYS OF CALCULATION)
Rsquare=function(m,o){
(cov(m,o)/sqrt(var(m)*var(o)))^2
}
R2=function(m,o){
y<-mean(m)
metric1<-(m - o) ^ 2
metric2<- (m-y)^2
s1<-sum(metric1)
s2<-sum(metric2)
ss<-s1/s2
1-s1/s2
}
DATA<-read.csv("DATA.csv",  header=T, stringsAsFactors=T)
str(DATA)
sum(is.na(DATA))
DATA$AMF.Duration.min <-as.numeric(DATA$AMF.Duration.min)
DATA$ Release.pH   <-as.numeric(DATA$ Release.pH)
DATA$ Release.conditions.Temperature  <- as.factor(DATA$ Release.conditions.Temperature)
#DATA PARTITION
## 1. DATA DEVELOPMENT (DD)& EXTERNAL VALIDATION (VS) DATASETS
set.seed(1821)
Partition<-createDataPartition(DATA$Drug.release.... ,p=0.87,list = FALSE)
DD<-DATA [Partition,]
VAL<-DATA[-Partition,]
str(DD)
str(VAL)
##write.csv(VAL, file="VS2.csv")
##write.csv(DD, file="DD.csv")
## TRAIN & TEST DATASETS
set.seed(1962)
inTrain<-createDataPartition(DD$Drug.release.... ,p=0.85,list = FALSE)
DATATrain<-DD[inTrain,]
DATATest<-DD[-inTrain,]
str(DATATrain)
str(DATATest)
#write.csv(DATATrain, file="DATA_TRAIN.csv")
#write.csv(DATATest, file="DATA_TEST.csv")
## PREPROCESS OF THE DATA
## TRAIN SET
DATA1Train<-DATATrain[,-1]
##preprocessParams1 <- preProcess(DATA1Train[,1:12], method=c("range"))
##print(preprocessParams1)
##DATA1Train_trans<-predict(preprocessParams1, DATA1Train[,1:12])
##DATA1Train<-cbind(DATA1Train_trans[1:12], DATA1Train[13] )
##DATA1Train[is.na(DATA1Train)]<--20
min<-min(DATA1Train $Drug.release....) ## 0
max<-max(DATA1Train$ Drug.release....) ##100
min
max
preprocessParams2 <- preProcess(DATA1Train[,1:13], method=c("range"))
preprocessParams21 <- preProcess(DATA1Train[,1:13], method=c("range"))
print(preprocessParams2)
DATA1Train<-predict(preprocessParams2, DATA1Train[,1:13])
DATA1Train[is.na(DATA1Train)]<--20
## TEST SET
DATA1Test<-DATATest[,-1]
DATA1Test<-predict(preprocessParams2, DATA1Test[,1:13])
DATA1Test[is.na(DATA1Test)]<--20
str(DATA1Train)
str(DATA1Test)
## EXTERNAL VALIDATION SET
VS<-VAL[,-1]
VS$AMF.Duration.min <-as.numeric(VS$AMF.Duration.min)
VS$ Release.pH   <-as.numeric(VS$ Release.pH)
VS$ Release.conditions.Temperature  <- as.factor(VS$ Release.conditions.Temperature)
VS<-predict(preprocessParams2, VS[,1:13])
VS[is.na(VS)]<--20
str(VS)
## ONE HOT ENCODING NEEDED
##OHE (ONE HOT ENCODING)
## 1.- Train set
dmyRTD<-dummyVars(Drug.release....~.,data=DATA1Train)
dmyRTD2<-dummyVars(~.,data=DATA1Train[,1:12])
jaqpotr::login.cred()
#LOAD PACKAGES
library(tidyverse)
library(dplyr)
library(magrittr)
library(mlbench)
library(caret)
library(caretEnsemble)
library(randomForest)
library(corrplot)
library(xgboost)
library(rpart)
library (rpart.plot)
library(WVPlots)
library(MASS)
library(neuralnet)
library(PerformanceAnalytics)
## R version 3.5.1
## SET WORKING DIRECTORY
#setwd("~/../Dropbox/DOXORUBICIN/NEW PROJECT DOX/DRUG RELEASE DATA/CODE_DOX/code_DOX_1a ") ### you can set your own working directory
#### FUNCTIONS FOR MODEL STATISTICS
rmse = function(m, o){
sqrt(mean((m - o)^2))
}
## Rsquared FUNCTION (2 DIFFERENT WAYS OF CALCULATION)
Rsquare=function(m,o){
(cov(m,o)/sqrt(var(m)*var(o)))^2
}
R2=function(m,o){
y<-mean(m)
metric1<-(m - o) ^ 2
metric2<- (m-y)^2
s1<-sum(metric1)
s2<-sum(metric2)
ss<-s1/s2
1-s1/s2
}
DATA<-read.csv("DATA.csv",  header=T, stringsAsFactors=T)
str(DATA)
sum(is.na(DATA))
DATA$AMF.Duration.min <-as.numeric(DATA$AMF.Duration.min)
DATA$ Release.pH   <-as.numeric(DATA$ Release.pH)
DATA$ Release.conditions.Temperature  <- as.factor(DATA$ Release.conditions.Temperature)
#DATA PARTITION
## 1. DATA DEVELOPMENT (DD)& EXTERNAL VALIDATION (VS) DATASETS
set.seed(1821)
Partition<-createDataPartition(DATA$Drug.release.... ,p=0.87,list = FALSE)
DD<-DATA [Partition,]
VAL<-DATA[-Partition,]
str(DD)
str(VAL)
##write.csv(VAL, file="VS2.csv")
##write.csv(DD, file="DD.csv")
## TRAIN & TEST DATASETS
set.seed(1962)
inTrain<-createDataPartition(DD$Drug.release.... ,p=0.85,list = FALSE)
DATATrain<-DD[inTrain,]
DATATest<-DD[-inTrain,]
str(DATATrain)
str(DATATest)
#write.csv(DATATrain, file="DATA_TRAIN.csv")
#write.csv(DATATest, file="DATA_TEST.csv")
## PREPROCESS OF THE DATA
## TRAIN SET
DATA1Train<-DATATrain[,-1]
##preprocessParams1 <- preProcess(DATA1Train[,1:12], method=c("range"))
##print(preprocessParams1)
##DATA1Train_trans<-predict(preprocessParams1, DATA1Train[,1:12])
##DATA1Train<-cbind(DATA1Train_trans[1:12], DATA1Train[13] )
##DATA1Train[is.na(DATA1Train)]<--20
min<-min(DATA1Train $Drug.release....) ## 0
max<-max(DATA1Train$ Drug.release....) ##100
min
max
preprocessParams2 <- preProcess(DATA1Train[,1:13], method=c("range"))
preprocessParams21 <- preProcess(DATA1Train[,1:12], method=c("range"))
jaqpotr::deploy.caret(trained.model = list(fitRF1A, fitXGB3A, model.neuralnet.caret2),
preprocess.model = list(preprocessParams21, dmyRTD2), ensemble.model = RFEnsembleXN,
replace = list("after", -20), ymax = 100, ymin = 0, url = http://localhost:8080/jaqpot/
)
jaqpotr::deploy.caret(trained.model = list(fitRF1A, fitXGB3A, model.neuralnet.caret2),
preprocess.model = list(preprocessParams21, dmyRTD2), ensemble.model = RFEnsembleXN,
replace = list("after", -20), ymax = 100, ymin = 0, url = "http://localhost:8080/jaqpot/" )
object <- jsonlite::fromJSON("C:/Users/user/Desktop/Jaqpot/R/json/doxorubicin.json")
dataset <- object$dataset
rawModel <- object$rawModel
additionalInfo <- object$additionalInfo
#################################
## Input retrieval from Jaqpot ##
#################################
# Get feature keys (a key number that points to the url)
feat.keys <-  dataset$features$key
# Get feature names (actual name)
feat.names <- dataset$features$name
# Create a dataframe that includes the feature key and the corresponding name
key.match <- data.frame(cbind(feat.keys, feat.names))
# Convert factor to string (feat.names is converted factor by data.frame())
key.match[] <- lapply(key.match, as.character)
# Initialize a dataframe with as many rows as the number of values per feature
rows_data <- length(dataset$dataEntry$values[,2])
df <- data.frame(matrix(0, ncol = 0, nrow = rows_data))
for(key in feat.keys){
# For each key (feature) get the vector of values (of length 'row_data')
feval <- dataset$dataEntry$values[key][,1]
# Name the column with the corresponding name that is connected with the key
df[key.match[key.match$feat.keys == key, 2]] <- feval
}
# Convert "NA" to NA
for (i in 1:dim(df)[1]){
for (j in 1:dim(df)[2]){
if(!is.na(df[i,j])){
if(df[i,j] == "NA"){
df[i,j] <- NA
}
}
}
}
# Extract the predicted value names
predFeat <- additionalInfo$predictedFeatures[1][[1]]
# Make the prediction using the model and the new data
# Note that the names of the dataframe must be the same with the original
###########################
## Model unserialization ##
###########################
mod <- unserialize(jsonlite::base64_dec(rawModel))
model <- mod$MODEL
preprocess <- mod$PREPROCESS
ensemble <- mod$ENSEMBLE
if (length(mod$extra.args)==1){
extra.args <- mod$extra.args[[1]]
}else{
extra.args <- NULL
}
# Replace NAs
replace <- additionalInfo$fromUser$replace
if(!is.null(replace)){
#Convert character to numeric
if(!is.na(as.numeric(replace[2]))){
replace.value <- as.numeric(replace[2])
}else{
replace.value <- replace[2]
}
}
####################
## Preprocessing ##
####################
# Do the NA substitution before preprocessing, if "before" is provided by the user
if(!is.null(replace)){
if(replace[1] == "before"){
for (i in 1:dim(df)[1]){
for (j in 1:dim(df)[2]){
if(is.na(df[i,j])){
df[i,j] <- replace.value
}
}
}
}
}
# If there is a preprocess stage apply it, else just predict
if(!is.null(preprocess)){
#if there is a preprocess model that is dummy vars, then retrieve factors
ModelForNames = NULL
for(i in 1:length(preprocess)){
if(attributes((preprocess[[i]]))$class == "dummyVars"){
ModelForNames <- preprocess[[i]]
# Retrive the original classes of the dataset's categorical vars
for (i in 1:dim(df)[2]){
#Retrieve levels of factor
if( attr(ModelForNames$terms, "dataClasses")[colnames(df)[i]] == "factor"){
df[,i] <- factor(df[,i], levels = ModelForNames$lvls[colnames(df)[i]][[1]])
}
}
}
}
# If there is no dummy vars, use the first model to retrieve factors
if(is.null(ModelForNames)){
# If it's ensemble, there are multiple models, so use one for getting the categorical variables
if(!is.null(ensemble)){
ModelForNames <- model[[1]]
}else{
ModelForNames <- model
}
# Retrive the original classes of the dataset's categorical vars
for (i in 1:dim(df)[2]){
#Retrieve levels of factor
if( attr(ModelForNames$terms, "dataClasses")[colnames(df)[i]] == "factor"){
df[,i] <- factor(df[,i], levels = ModelForNames$xlevels[colnames(df)[i]][[1]])
}
}
}
#Data preprocessing
for (i in 1:length(preprocess)){
preprocess.method <- preprocess[[i]]
preprocessData <- predict(preprocess.method, df)
df <- preprocessData
}
}
# Do the NA substitution after preprocessing, if "after" is provided by the user
if(length(replace) == 2){
if(replace[1] == "after"){
for (i in 1:dim(df)[1]){
for (j in 1:dim(df)[2]){
if(is.na(df[i,j])){
df[i,j] <- replace.value
}
}
}
}
}
################
## Prediction ##
################
# Retrieve ensemble column names
ensemble.vars <- additionalInfo$fromUser$ensemble.vars
if(!is.null(ensemble)){
stacking <-  matrix(rep(NA, length(model)* dim(df)[1]), ncol = length(model))
for (i in 1:length(model)){
# Select the model to use
UseModel <- model[[i]]
stacking[,i] <- predict(UseModel, df)
}
# Convert the stacked predictions matrix to dataframe and name the columns of the dataset appropriately
stacking <- as.data.frame(stacking)
colnames(stacking) <- ensemble.vars
# Predict using the ensemble model
predictions <- predict(ensemble, stacking)
}else{
predictions <- predict(model, df)
}
######################
## Detransformation ##
######################
#Apply detransformation
ymin <- additionalInfo$fromUser$ymin
ymax <- additionalInfo$fromUser$ymax
if(length(ymin) == 1 && length(ymax) ==1){
for (i in 1:length(predictions)){
predictions[i] <- predictions[i]*(ymax-ymin)+ ymin
}
}
# Not offered to users (custom detransformation)
if (!is.null(extra.args)){
predictions <- extra.args(predictions)
}
##################################
## Name and return predictions  ##
##################################
for(i in 1:length(predictions)){
prediction<- data.frame(predictions[i])
colnames(prediction)<- predFeat
if(i==1){lh_preds<- list(jsonlite::unbox(prediction))
}else{
lh_preds[[i]]<- jsonlite::unbox(prediction)
}
}
datpred <-list(predictions=lh_preds)
datpred
setwd("C:/Users/user/Documents/GitHub/GenericR")
devtools::document()
install.packages("truncnorm")
install.packages("party")
install.packages("neighbr")
devtools::document()
devtools::build()
